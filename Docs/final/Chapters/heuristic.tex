\cleardoublepage
\chapter{Geometric Disk Cover}
\label{chap:approx}
%\lhead{Chapter \ref{chap:approx}. \emph{\nameref{chap:approx}}} % This is for the header
\vspace{-15pt}
The goal of the geometric disk cover problem is to, given a set $N$ of points and a minimum distance $d$, find the smallest number $k$ of circles of radius $d$ centred around points in $N$ such that no point is left uncovered. This chapter describes an efficient way to approximate the optimal solution to this new problem. The number of points selected by the algorithm is bound above by $m \ln {n}$ points, where $m$ is the optimal value of $k$. The algorithm is then experimentally tested for performance in a set of benchmark instances.

Finally, some speed-up heuristic methods are explored. The heuristic algorithms are also experimentally tested for performance and compared to the regular approximation algorithm.

\section{Approximation Algorithm}
Calculating the optimal solution for Geometric Disk Cover is an \emph{NP-hard} problem \cite{gdccomplex}. However, there are ways of finding a reasonable approximation to the optimal solution in a short amount of time. Briefly, this approach requires two steps. The first step is the Proximity Graph Building step, which builds a graph connecting all pairs of points that are close together. The second step is a Set Cover step, which uses an approximation algorithm to cover the graph built in the former step. The following sections describe these steps in more detail.

\subsection{Proximity Graph Construction}
The first step is to create a graph connecting all pairs points that are within a distance of each other. This means constructing a graph in which every point is a vertex, and every edge connects two vertices whose points are within the given distance, also known as a proximity graph \cite{proximity}. The na√Øve approach to building this graph would be to test all the distances between each pair of points, taking $\bigo(n^2)$ operations. Alternatively a line sweep algorithm or a series of range searches on a \kdtree could be used to speed up the process. The line sweep algorithm would require a $\bigo(n \log{n})$ sorting algorithm, followed by $\bigo(n^2)$ comparisons. However, the latter is limited by the number of points within the sliding window, which should only contain a fraction of the total number of points on the map, since the distance chosen is a fraction of its dimensions. The \kdtree range search, on the other hand, requires a $\bigo(n \log{n})$ construction of a \kdtree using a median of medians algorithm. This is followed by $n$ queries consisting of finding the points within a square of side $2d$ centred around each of the points. Each one of these operations take $\bigo(\sqrt{N}+m)$ time, where $m$ is the number of returned points \cite{kdrange}. Even though the complexity of the \kdtree range search is theoretically faster than the line sweep method, it comes with an extra overhead of handling a more complex structure. As such, both methods are analysed from an experimental point of view in this thesis. After this operation, the graph connecting all neighbours is built. Figure \ref{fig:aa1} illustrates one such graph.

\input{Figures/apx1}

This graph is represented via adjacency lists. The adjacency lists have an advantage over an adjacency matrix as linked lists can be used to considerably reduce the memory footprint in sparse graphs. It also allows for faster sequential access to the neighbours of any given point. These linked lists share similarities to the half-edge structure, in which each node represents a unidirectional edge that contains a pointer to its counterpart.

It is noteworthy that the graph can reach $\bigo(n^2)$ edges if the points are all very close together and/or the minimum distance is large enough. This case coupled with the fact that the adjacency linked lists occupy more space per edge than an adjacency matrix, the space needed to keep the graph in memory can be potentially too high for some machines to handle larger instances of the problem. 

\subsection{Set Cover}
The second step to the algorithm is to chose a small subset of vertices whose neighbours unions are equal to the whole set of vertices of the graph. This is known as \emph{set cover}, and its solution can be approximated using a greedy approach \cite{approxalgos}. Starting with the whole uncovered graph, the point with the largest number of uncovered neighbours is selected. This is done iteratively until no uncovered points remain on the graph. This approach approximates the number of points to within $m \ln{n}$ points, were $m$ is the optimal number.

At each step of this algorithm, one point $p$ and all its neighbours and respective edges must be removed from the graph. This is done by iterating through the adjacency lists of the neighbours of $p$ and deleting all the connections to their neighbours (second-degree neighbours of $p$). This operation takes exactly $\bigo(n)$ time where $n$ is proportional to the number of edges to be deleted. Figure \ref{fig:aa2} illustrates the graph after the first iteration:

\input{Figures/apx2}

After all the points are covered, the collection of selected centres makes the subset of centroids that is displayed as the final output. The cardinality of this set does not exceed the approximation as described above.

\input{Figures/apx3}
\subsection{Results}
This section reports both approaches (\kdtrees and line sweeping) to the graph building portion of the algorithm. The implementations differ from each other only in the range search algorithm they use to build the proximity graph. This is done because the graph building stage has the highest time complexity, and is therefore expected to be the bottleneck. In fact, running a code profiler shows that the graph building stage takes 98\% of the CPU-time, with the time taken to run the set cover algorithm stage being negligible.

For the tests in this section, the given value of $d$ is the minimum distance between points, therefore radius of the covering disks. It is given as a percentage of the largest dimension of the window, as this value is easier to adjust and analyse by a human user.
Figure \ref{fig:sweden_dist} shows the outputs for different values of $d$ for a set of points of interest in Sweden. The data was obtained from the \href{http://www.math.uwaterloo.ca/tsp/world/swpoints.html}{University of Waterloo, Ontario, Canada}\cite{waterloo_sweden}.

\input{Figures/sweden_dist}
By analysing images of a few real-world examples, the values for the minimum distance $d$ were agreed to be given as a value between 10\% and 15\% of the largest dimension of the window. These values give the subsets that were deemed as the easiest to look at without losing sense of shape of the area. The performance tests still use the value of 20\% to test the algorithms under less favourable conditions, since the larger the value of $d$, the denser and more time is required to build the proximity graph. 

Since this approach is more interesting for the web application from a coputational time point of view, a new set of benchmark tests were defined. Both uniform and clustered inputs are tested. The uniform inputs are a simple collection of $N$ points whose Cartesian coordinates are randomly chosen between $0$ and $100$. The clustered inputs are generated by choosing a random number of points $s$ between 10 to 20 points. Each of these points acts as a centre and is chosen by randomly generating two Cartesian coordinates between 0 and 100. Around each of these, $N/s$ points were generated by randomly choosing an angle $\Theta$ (between $0$ and $2\pi$) and a distance $\rho$ (between $10$ and $20$), which represent the polar coordinates around their respective central point. These tests generate circular clusters, with more density towards the centre. Each test is performed 30 times, with shared seeds between the algorithms. The times listed do include the input scanning. All algorithms are implemented using ANSI C89 and compiled using \emph{gcc 5.1.0}. The programs were ran on a machine with a Intel i7 Dual-Core, 2GHz processor, with a 8 GB, 1600 MHz memory and Arch Linux 3.14.4 x64 as its operating system.

The first test performed benchmarked both line sweep and \kdtree range search algorithms for the proximity graph construction. The results are shown in Figure \ref{fig:ls_kd_t}. As the Figure shows, the line sweep method performs faster in both the uniform and the clustered data. The lower expected complexity of the \kdtrees does not compensate the larger overhead in their construction. It can also be seen that both algorithms become slower the larger the number of points, as was expected.

Building a proximity graph with 30000 vertices has the potential to need $30000^2$ edges, making these algorithms very memory intensive. Even with adjacency lists instead of an adjacency matrix, the inputs can generate a instance where every point must be connected to every other point. In this case, the advantage of the adjacency lists for sparse graphs is lost, and the program exceeds the RAM limit given by the operating system. This occurs more frequently for larger values of $d$ and $N$, with the smallest case recorded being of $N=25000$ and $d=0.2$ on a clustered input that generated most clusters on top of each other, concentrating most points within a circle of a small radius, where most of them had over 20000 neighbours. However this case is somewhat rare, and only occurs under very specific circumstances and did not occur during the tests displayed. Figure \ref{fig:ls_kd_s} shows the amount of memory used by both algorithms on the tests performed. 
The lines for both algorithms overlap, since applying both methods to any given case gives the same proximity graph, as it is unique. 

However, the final set may not be the same. The coverage algorithm picks the points with most neighbours. Since two different points may have the same number of neighbours, and the two algorithms sort the points in two different ways, there may occur a case where the sets are completely different, based on which point is selected first in case of a draw. The values still fall bellow the upper bound for the approximation factor, and no algorithm should have the advantage over the other. Figure \ref{fig:ls_kd_k} shows the number of points selected by each algorithm in the same instances as above. The figure shows that the number of selected points $k$ does not seem to vary very much with $N$ for the same values of $d$. In fact, the growth of the value of $k$ seems to suggest an asymptotic approach to a fixed value. This happens because there is a limit to how many circles can be placed in an area without any of them containing the centres of the others. 

\input{Figures/kd_ls_t}
\input{Figures/kd_ls_s}
\input{Figures/kd_ls_k}

The process of finding the best disposition of circles of the same radius in a given area is known as circle packing. The number of circles in a circle packing instance for a square area is given by the ratio between the circles' radii and the width of the square enclosing them. Since the value for $d$ is given as a fraction of the size of the window, the bound values of $k$ can be calculated for the various values of $d$. A detailed explanation of how to calculate the values for the upper bound is given in Appendix \ref{ann:packing}. The values indicate that $k$ should not be larger than $\{128,59,36\}$, for the given values $d=\{0.1,0.15,0.2\}$, respectively. With the exception of the latter, these values are yet to be proven to be optimal, but are not expected to be very different, and serve as a good estimate of the maximum number of points that can be selected.

These results suggest that the best algorithm to use is the line sweep algorithm, as it takes less time and the same memory to compute similar results.
	
\section{Heuristic Speed-ups}
The approximation algorithm runs rather efficiently, taking under 1 second for 30000 points for the uniform inputs, and under 3 seconds for the clustered inputs. Nonetheless, they are very memory intensive, which could present an issue for larger instances. This result can be improved by employing some heuristic filtering methods to the inputs. By using these approaches, however, the guarantee of the quality of the approximation is lost. Nevertheless, the results may indicate if the heuristic algorithms should still be considered to handle larger numbers of points.

\subsection{Random Sampling}
The simplest method to speed-up the algorithms is to simply ignore a randomly chosen subset of those points. With the smaller sample of points, the algorithm should run faster and use less memory space. If the points are removed uniformly, then the overall distribution should still be kept. Two versions of this method were tested. In the first, only one in every five points was considered by the algorithm, whereas in the second only one in every ten were. Figure \ref{fig:ls_rs_t} shows the time difference between the regular line sweep and the randomly sampled input algorithms.
The graph proves that the sampling is indeed faster. This speed-up is caused by the lower number of starting points for the algorithm to process. 

The smaller number also affects the memory needed by these algorithms, which require less space to store the graph. This happens because the number of edges tends to grow quadratically in relation to the number of points, meaning that a set with $1/5$th of the points would be expected to have around $1/25$th of the edges. Figure \ref{fig:ls_rs_s} shows the space used by the programs. This time, no cases that exceeded the memory limit were recorded. The figure shows that the random samples use the space expected above. Where the line sweep algorithm uses an average of 4164.93Mb for the clustered inputs, the random samples use 167.83Mb for the $1/5$th sample version and 41.44Mb for the $1/10$ sample version. The uniform inputs had a similar result, with the line sweep algorithm using an average of 3729.71Mb for the largest inputs, and the random sample algorithms using 149.17Mb for the $1/5$th sample version and 37.31Mb in the $1/10$th sample version, matching our initial prediction of $1/25$th and $1/100$th the amount of memory used by the line sweep algorithm to the second decimal case. 

\input{Figures/ls_rs_t}
\input{Figures/ls_rs_s}
\input{Figures/ls_rs_k}

Upon closer inspection, however, it can be noted that the resulting set is not optimal. Figure \ref{fig:ls_rs_k} shows that the number of selected points is inferior.
This can be explained by the fact that the algorithm is not accounting for all points. In fact, whenever the original set has one isolated point, away from any other by more than the coverage distance, it has a 4 out of 5 chance of being discarded and unaccounted for (or 9 out of 10, depending on the version of the algorithm). This means that not all points are being covered, and the number of selected points decreases. Figure \ref{fig:ls_rs_greece} shows this effect in a real map.

\input{Figures/ls_rs_greece}

As shown, the sampled subset does not cover the most isolated points. In fact, the westernmost point in the map is not present in the sample and, therefore, is not covered. This method is not suitable for the initial requirements, since very isolated points have only a small chance of being represented.

\subsection{Two-phase filtering}

A solution tat avoids the flaws of the random sampling algorithm is to perform two passes of the approximation algorithm for the geometric disk cover problem. The first pass over the points is done with a very small radius $d^\prime$, since using a small distance speeds the algorithm up, as previously seen in Figure \ref{fig:ls_rs_t}. This can be done very quickly, since the range search only has to look in a very small area. This means that many of the points that are close to each other are discarded, but a representative neighbour is always left in its place. Isolated points are then never unaccounted for. The resulting set is much smaller than the original, whilst still keeping a representativeness degree. The second pass, now with the full coverage distance $d$ takes advantage of the much smaller set of points for the speed-up. 

Despite accounting for isolated points, the resulting set does not necessarily cover all the points with disks of radii $d$. Because the first pass transforms the set into disks of radii $d^\prime$, and the second pass only considers their centre point as the measure of cover, then there can be points that are further away from the centroids than $d$, the maximum distance being $d+d^\prime$, as shown in Figure \ref{fig:bp_error}. This means that by the strict definition of having all points covered by the disks, there are points left uncovered, but only by a guaranteed margin.

\input{Figures/bp_error}

The \emph{CPU} times for three instances of the algorithm can be seen in Figure \ref{fig:ls_bp_t}. The values of the first pass distance $d^\prime$ are given as a percentage of the final pass distance $d$. One of the instances uses $d^\prime=0.05d$, another $d^\prime=0.1d$ and the other uses $d^\prime=0.2d$.

As it can be seen, the two-phase filtering is more efficient than the simple line sweep algorithm and does not seem to vary a lot with the total number of points. The first pass eliminates a very large number of redundant points, leaving a similar resulting set for different values of $N$. The intermediate subset is still representative of the original. 

The smaller distance requires less edges to be created, and the proximity graph for the first phase is uses less space than a one-phase algorithm does. The proximity graph for the second phase benefits from having less points, thus requiring less edges. Since this algorithm calculates two graphs, both of which sparser than the complete on in the Line Sweep and \kdtree algorithms, the memory limit was not an issue for the inputs tested. Figure \ref{fig:ls_bp_s} shows the memory footprint of the three versions of the algorithm. These algorithms are less predictable concerning the size of the graphs, since they depend a lot from local density of points for the first phase. While they do not have as small a footprint as the random sampling, they still use considerably less than the line sweep and \kdtree range search. 

These algorithms do not share the issue of leaving isolated points completely uncovered like the random sampling algorithms do, returning better representative subsets. Figure \ref{fig:ls_bp_k} shows the variation in size of the final subset chosen by the two-phase filtering algorithms.

The graph shows that despite the two-phase algorithms returning lower numbers of $K$, they are not as low as the random sampling. 

\input{Figures/ls_bp_t}
\input{Figures/ls_bp_s}
\input{Figures/ls_bp_k}

This result has some effect on the quality, but it is not nearly as noticeable as the results from the random sampling. Figure \ref{fig:bp_sweden} shows the compared output between the original line sweep algorithm, and the different versions of the two-phase algorithm, as well as their intermediate phases.


This final result shows that the two-phase filter, especially with $d^\prime=0.1d$, can yield very approximate results at a fraction of the time and space, thus making a good candidate to use with inputs larger than the regular Line Sweep method can handle.

\input{Figures/bp_sweden}

\section{Region Panning and Zooming}
One of the requirements established for the final algorithm it that it should be able to handle a translation motion in the region and starting point set. This means that in case the new region intersects the previous one, it is important for the user that the points already in display are kept selected, so that each move does not cause the user to lose track of the movement. As such, any selected centroids inside the intersection of the two regions, and all the points covered by them, should be kept unchanged, even if it means losing the guaranteed approximation to the optimal value. Each of the approaches described earlier can be modified to ensure that this property is met. 

To handle the panning case, each instance of the program receives the centroids chosen in the previous region. To ensure that the points are still selected is a matter of artificially increasing their number of neighbours by $N$, then compute the problem regularly.  Since the number of points from the old region is bound from above by at most 128, the new number of points is relatively insignificant for instances of 10000 points and more, and the algorithms show no significant difference in performance. Since the covering distance does not change, the old centroids are not close enough together to cover each other, otherwise they would have done so in the old region. This solves the panning issue without needing to add much complexity to the algorithms. In the case of multiple phase filters, the weight of the selected points must be artificially increased in all the phases, to ensure they are kept selected.

\input{Figures/panning}

In the case of zooming, the scale changes, and so does the minimum distance. The points selected previously are relatively more close together in the new window, and they shift towards the centre of the region. One of the solutions is to simply discard the previous selected points, since they do not have the same representation properties they did in the previous region. Another solution is to do the same process as for the panning. By artificially inflating the weight of the established points, they are given priority over the new window's points. Since the points are now closer together in the window, they may cover each other, resulting in some prioritised points not being selected. 

\input{Figures/zooming}

\section{Discussion}
The approximation algorithms in this chapter meet the efficiency requirements established for the web application, and are also able to handle panning and zooming motions without any complexity. In the case the test inputs prove to still be too large to be handled by the approximation algorithms, a good alternative is found in the two-phase filtering approach.

A solution for even larger set of points may require more filter phases, but for the inputs given, no such solution was needed. Furthermore, since the two-phase filtering approaches show similar CPU-times to the random sampling approaches, while showing fewer critical shortcomings, there should be no advantage in using random sampling.