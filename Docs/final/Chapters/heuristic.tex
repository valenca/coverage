\cleardoublepage
\chapter{Geometric Disk Cover}
\label{chap:approx}
%\lhead{Chapter \ref{chap:approx}. \emph{\nameref{chap:approx}}} % This is for the header

The goal of the geometric disk cover problem is to, given a set $N$ of points and a minimum distance $d$, find the smallest number $k$ of circles of radius $d$ centred around points in $N$ such that no point is left uncovered. This chapter describes an efficient way to approximate the optimal solution to this new problem. The number of points selected by the algorithm is bound above by $m \ln {n}$ points, where $m$ is the optimal value of $k$.

\section{Approximation Algorithm}
Calculating the optimal solution for Geometric Disk Cover is a \emph{NP-hard} problem \cite{gdccomplex}. However, there are ways of finding a reasonable approximation to the optimal solution in a short amount of time. Briefly, this approach requires two steps. The first step is the Proximity Graph Building step, which builds a graph connecting all pairs of points that are close together. The second step is a Set Cover step, which uses an approximation algorithm to cover the graph built in the former step. The following sections describe these steps in more detail.

\subsection{Proximity Graph Construction}
The first step is to create a graph connecting all pairs points that are within a distance of each other. This means constructing a graph in which every point is a vertex, and every edge connects two vertices whose points are within the given distance, also known as a proximity graph \cite{proximity}. The na√Øve approach to building this graph would be to test all the distances between each pair of points, taking $\bigo(n^2)$ operations. Alternatively a line sweep algorithm or a series of range searches on a \kdtree could be used to speed up the process. The line sweep algorithm would require a $\bigo(n \log{n})$ sorting algorithm, followed by $\bigo(n^2)$ comparisons. However, the latter is limited by the number of points within the sliding window, which should only contain a fraction of the total number of points on the map, since the distance chosen is a fraction of its dimensions. The \kdtree range search, on the other hand, requires a $\bigo(n \log{n})$ construction of a \kdtree using a median of medians algorithm. This is followed by $n$ queries consisting of finding the points within a square of side $2d$ centred around each of the points. Each one of these operations take $\bigo(\sqrt{N}+m)$ time, where $m$ is the number of returned points \cite{kdrange}. Even though the complexity of the \kdtree range search is theoretically faster than the line sweep method, it comes with an extra overhead of handling a more complex structure. As such, both methods are analysed from an experimental point of view in this thesis. After this operation, the graph connecting all neighbours is built. Figure \ref{fig:aa1} illustrates one such graph.

\input{Figures/apx1}

This graph is represented via adjacency lists. The adjacency lists have an advantage over an adjacency matrix as linked lists can be used to considerably reduce the memory footprint in sparse graphs. It also allows for faster sequential access to the neighbours of any given point. These linked lists share similarities to the half-edge structure, in which each node represents a unidirectional edge that contains a pointer to its counterpart.

It is noteworthy that the graph can reach $\bigo(n^2)$ edges if the points are all very close together and/or the minimum distance is large enough. This case coupled with the fact that the adjacency linked lists occupy more space per edge than an adjacency matrix, the space needed to keep the graph in memory can be potentially too high for some machines to handle larger instances of the problem. 

\subsection{Set Cover}
The second step to the algorithm is to chose a small subset of vertices whose neighbours unions are equal to the whole set of graph's vertices. This is known as \emph{set cover}, and its solution can be approximated using a greedy approach \cite{approxalgos}. Starting with the whole uncovered graph, the point with the largest number of uncovered neighbours is selected. This is done iteratively until no uncovered points remain on the graph. This approach approximates the number of points to within $m \ln{n}$ points, were $m$ is the optimal number.

At each step of this algorithm, one point $p$ and all its neighbours and respective edges must be removed from the graph. This is done by iterating through the adjacency lists of the neighbours of $p$ and deleting all the connections to their neighbours (second-degree neighbours of $p$). This operation takes exactly $\bigo(n)$ time where $n$ is proportional to the number of edges to be deleted. Figure \ref{fig:aa2} illustrates the graph after the first iteration:

\input{Figures/apx2}

After all the points are covered, the collection of selected centres makes the subset of centroids that is displayed as the final output. The cardinality of this set does not exceed the approximation as described above.

\input{Figures/apx3}
\subsection{Results}
This section reports both approaches (\kdtrees and line sweeping) to the graph building portion of the algorithm. The implementations differ from each other only in the range search algorithm they use to build the proximity graph. This is because the graph building stage has the highest time complexity, and is therefore expected to be the bottleneck. In fact, running a code profiler shows that the graph building stage takes 98\% of the CPU-time, with the time taken to run the set cover algorithm stage being negligible.

For the tests in this section, the given value of $d$ is the minimum distance between points, therefore radius of the covering disks. It is given as a percentage of the largest dimension of the window, as this value is easier to adjust and analyse by a human user.
Figure \ref{fig:sweden_dist} shows the outputs for different values of $d$ for a set of points of interest in Sweden.

\input{Figures/sweden_dist}
By analysing images of a few real-world examples, the values for the minimum distance $d$ were agreed to be given as a value between 10\% and 15\% of the largest dimension of the window. These values give the subsets that were deemed as the easiest to look at without losing sense of shape of the area. The performance tests still use the value of 20\% to test the algorithms under less favourable conditions, since the larger the value of $d$, the denser and more time is required to build the proximity graph. 

Since the algorithms in this chapter are more efficient than the ones in the previous chapter, new tests were needed to benchmark them. These tests use random inputs to benchmark the performance of the algorithms. Both uniform and clustered inputs are tested. The uniform inputs are a simple collection of $N$ points whose Cartesian coordinates are randomly chosen between $0$ and $100$. The clustered inputs are generated by choosing a random number of points $s$ between 10 to 20 points. Each of these points acts as a centre and is chosen by randomly generating two Cartesian coordinates between 0 and 100. Around each of these, $N/s$ points were generated by randomly choosing an angle $\Theta$ (between $0$ and $2\pi$) and a distance $\rho$ (between $10$ and $20$), which represent the polar coordinates around their respective central point. These tests generate circular clusters, with more density towards the centre.

Each test is performed 30 times, with shared seeds between the algorithms. The times listed do include the input scanning. All algorithms are implemented using ANSI C89 and compiled using \emph{gcc 5.1.0}. The programs were ran on a machine with a Intel i7 Dual-Core, 2GHz processor, with a 8 GB, 1600 MHz memory and Arch Linux 3.14.4 x64 as its operating system.

The first test performed benchmarked both line sweep and \kdtree range search algorithms for the proximity graph construction. The results are shown in Figure \ref{fig:ls_kd_t}.

\input{Figures/kd_ls_t}

As Figure \ref{fig:ls_kd_t} shows, the line sweep method performs faster in both the uniform and the clustered data. The lower expected complexity of the \kdtrees does not compensate the larger overhead in their construction. It can also be seen that both algorithms become slower the larger the number of points, as was expected. It should be noted that the algorithm is very memory intensive. Even with adjacency lists instead of a matrix, the inputs can generate a instance where every point must be connected to every other point. In this case, the advantage of the adjacency lists for sparse graphs is lost, and the program exceeds the RAM limit given by the Operating system. This occurs more frequently for larger values of $d$, and $N$, with the smallest case recorded being of $N=25000$ and $d=0.2$ on a clustered input that generated most clusters on top of each other, concentrating most points within a circle of a small radius, where most of them had over 20000 neighbours. 

Applying both methods to any given case gives similar results. The coverage algorithm picks the points with most neighbours. Since two different points may have the same number of neighbours, and the two algorithms sort the points in two different ways, there may occur a case where the sets are completely different, based on which point is select first in case of a draw. The values still fall bellow the upper bound for the approximation factor, and no algorithm should have the advantage. Figure \ref{fig:ls_kd_k} shows the number of points selected by each algorithm in the same instances as above.

\input{Figures/kd_ls_k}

Figure \ref{fig:ls_kd_k} shows  that the number of selected points does not seem to change for the same values of $d$. In fact, the growth of the value of $k$ seems to suggest an asymptotic approach to a fixed value. This happens because there is a limit to how many circles can be placed in an area without any of them containing the centres of the others. 
	
The process of finding the best disposition of circles of the same radius in a given area is known as circle packing. The number of circles in a circle packing instance for a square area is given by the ratio between the circles' radii and the width of the square enclosing them. Since the value for $d$ is given as a fraction of the size of the window, the bound values of $k$ can be calculated for the various values of $d$. A detailed explanation of how to calculate these values is given in Appendix \ref{ann:packing}. The values indicate that $k$ should not be larger than $\{128,59,36\}$, for the given values $d=\{0.1,0.15,0.2\}$, respectively. With the exception of the latter, these values are yet to be proven to be optimal, but are not expected to be very different, and serve as a good estimate of the maximum number of points that can be selected.

Figure \ref{fig:ls_kd_k} also shows that no algorithm has the advantage on the number of points chosen. In fact, the results overlap each other almost completely. This suggests that the best algorithm to use is the line sweep algorithm, as it takes less time to compute similar results.

\section{Heuristic Speed-ups}
The approximation algorithm runs rather efficiently, taking under 1 second for 30000 points for the uniform inputs, and under 3 seconds for the clustered inputs. This result can be improved by employing some heuristic filtering methods to the inputs. However, by using these approaches, the guarantee of the quality is lost. Nevertheless, the results mean that the algorithm should be considered to handle larger numbers of points.
\subsection{Sampling}
The simplest method to speed-up the algorithms is to simply ignore a randomly chosen subset of those points. With the smaller sample of points, the algorithm should run faster. If the points are removed uniformly, then the shape should still be kept.

Figure \ref{fig:ls_rs_t} shows the time difference between the regular line sweep and the sampled input.

\input{Figures/ls_rs_t}

The graph in Figure \ref{fig:ls_rs_t} proves that the sampling is indeed faster, no cases exceeded the memory limit. However, upon closer inspection, it can be noted that the result is not optimal. Figure \ref{fig:ls_rs_k} shows that the number of selected points is inferior:

\input{Figures/ls_rs_k}

This can be explained by the fact that the algorithm is not accounting for all points. In fact, the number of cases where a point is left isolated in the full set algorithms is decreased by the factor of the sampling in these algorithms. This means that not all points are being covered, and the number of selected points decreases. Figure \ref{fig:ls_rs_greece} shows this effect in a real map:

\input{Figures/ls_rs_greece}

As shown, the sampled subset does not cover the most isolated points. In fact, the westernmost point in the map is not present in the sample and, therefore, is not covered. This method is not suitable for the initial requirements, since very isolated points have only a small chance of being represented.

\subsection{Two-phase filtering}
A solution to the random sampling algorithm problems is to perform two passes of the approximation algorithm for the geometric disk cover problem. The first pass over the points is done with a very small radius. This can be done very quickly, since the range search only has to look in a very small area. Using a small distance speeds the algorithm up, as seen in Figure \ref{fig:ls_rs_t}. This means that many of the points that are close to each other are discarded, but a representative neighbour is left in its place. Isolated points are not discarded. The resulting set is much smaller than the original, whilst still keeping a representativeness degree. The second pass, with the final intended distance now takes advantage of the much smaller set of points for the speed up. The \emph{CPU} times for two instances of the algorithm can be seen in Figure \ref{fig:ls_bp_t}. The values of the first pass distance $d^\prime$ are given as a percentage of the final pass distance $d$. One of the instances uses $d^\prime=0.05d$, another $d^\prime=0.1d$ and the other uses $d^\prime=0.2d$.

\input{Figures/ls_bp_t}

As it can be seen, the two-phase filtering is very fast and does not seem to vary a lot with the total number of points. This is because the first pass eliminates a very large number of points, and the resulting set is very similar for different values of $N$, since the intermediate subset is still representative of the original. Since this algorithm calculates two graphs, very much sparser than the complete on in the Line Sweep and \kdtree algorithms, the memory limit was not an issue for the inputs tested. Figure \ref{fig:ls_bp_k} shows the variation in size of the final subset chosen by the two-phase filtering.

\input{Figures/ls_bp_k}

The graph shows that despite the two-phase algorithms returning lower numbers of $K$, they are not as low as the random sampling. The resulting set does not necessarily cover all the points with disks of radii $d$. Because the first pass transforms the set into disks of radii $d^\prime$, and the second pass only considers their centre point as the measure of cover, then there can be points that are further away from the centroids than $d$, the maximum distance being $d+d^\prime$, as show in Figure \ref{fig:bp_error}.

\input{Figures/bp_error}

This result has some effect on the quality, but it is not nearly as noticeable as the results from the random sampling. Figure \ref{fig:bp_sweden} shows the compared output between the original line sweep algorithm, and the different versions of the two-phase algorithm, as well as their intermediate phases.


This final result shows that the two-phase filter, especially with $d^\prime=0.1d$, can yield very approximate results at a fraction of the time, thus making a good candidate to use with inputs larger than the regular Line Sweep method can handle.

\input{Figures/bp_sweden}

\section{Region Panning and Zooming}
One of the requirements established for the final algorithm it that it should be able to handle a translation motion in the region and starting point set. This means that in case the new region intersects the previous one, it is important for the user that the points already in display are kept selected, so that each move does not cause the user to lose track of the movement. As such, any selected centroids inside the intersection of the two regions, and all the points covered by them, should be kept unchanged, even if it means losing the guaranteed approximation to the optimal value. Each of the approaches described earlier can be modified to ensure that this property is met. 

To handle the panning case, each instance of the program receives the centroids chosen in the previous region. To ensure that the points are still selected is a matter of artificially increasing their number of neighbours by $N$, then compute the problem regularly.  Since the number of points from the old region is bound from above by at most 128, the new number of points is relatively insignificant for instances of 10000 points and more, and the algorithms show no significant difference in performance. Since the covering distance does not change, the old centroids are not close enough together to cover each other, otherwise they would have done so in the old region. This solves the panning issue without needing to add much complexity to the algorithms. In the case of multiple phase filters, the weight of the selected points must be artificially increased in all the phases, to ensure they are kept selected.

\input{Figures/panning}

In the case of zooming, the scale changes, and so does the minimum distance. The points selected previously are relatively more close together in the new window, and they shift towards the centre of the region. One of the solutions is to simply discard the previous selected points, since they do not have the same representation properties they did in the previous region. Another solution is to do the same process as for the panning. By artificially inflating the weight of the established points, they are given priority over the new window's points. Since the points are now closer together in the window, they may cover each other, resulting in some prioritised points not being selected. 

\input{Figures/zooming}

\section{Discussion}
The approximation algorithms in this chapter meet the efficiency requirements established for the web application, and are also able to handle panning and zooming motions without any complexity. In the case the test inputs prove to still be too large to be handled by the approximation algorithms, a good alternative is found in the two-phase filtering approach.

A solution for even larger set of points may require more filter phases, but for the inputs given, no such solution was needed. Furthermore, since the two-phase filtering approaches show similar CPU-times to the random sampling approaches, while showing fewer critical shortcomings, there should be no advantage in using random sampling.